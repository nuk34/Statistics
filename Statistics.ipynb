{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ad8d3c4-75e5-4071-b724-14609003647d",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f18684-dd21-4dcd-82b5-0396b7ef6f15",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c9eba6-d127-4dc9-bd09-6549aefa5bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee663872-a615-41f7-8c1d-0533301a7914",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "plt.rcParams[\"savefig.facecolor\"] = (0.0, 0.0, 0.0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbdee38-a53c-44fc-9ef5-08e24323d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601902e0-b3d4-4566-9ebc-974c81062843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707d63f5-9bc6-48c3-be81-0a3390c981a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ac5a5e-1eb5-4f1e-9ec9-fc7ce1d7c7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.stats import poisson, t, norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b147df4f-a712-42f0-a5b4-1cf2eb4ab614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4e79a3-c66c-4fb6-a822-26d23fa5eae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0028e96-6768-410e-80dc-40adfe90a288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05de673-f4e4-4561-8b29-49a1aa856dcf",
   "metadata": {},
   "source": [
    "## random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42da9b29-ccfc-44a4-9a4a-8da2bcfe8060",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=20071973)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d71e326-1547-40a2-878a-5d659600e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000000\n",
    "\n",
    "tiles = 100000\n",
    "\n",
    "mu = N/tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249d7f63-9ae8-44a0-ba0f-87a855285d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "f'N = {N:1.0e}, t = {tiles:1.0e}, mu = {mu:2.0f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44f1908-7e21-4e1b-9595-b18f0800daca",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e696ff0-2393-4606-9b14-3c6593719144",
   "metadata": {},
   "outputs": [],
   "source": [
    "## draw random numbers, which are the tile indices\n",
    "random_numbers = rng.integers(0, tiles, size = N)\n",
    "\n",
    "## count the hits in each tile\n",
    "counts_in_tiles = np.unique_counts(random_numbers).counts\n",
    "\n",
    "## some tiles were never hit, [tiles - len(counts_in_tiles)] have zero counts\n",
    "## extend the array with this number of zeros\n",
    "counts_in_tiles = np.append(counts_in_tiles, np.zeros(tiles - len(counts_in_tiles), dtype = counts_in_tiles.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aa5a5c-e335-4cf2-8b60-4145aa85d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(counts_in_tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ee716-75a8-4e0f-b30c-5882baa008b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng.shuffle(counts_in_tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bacb44-3b7a-4885-a4f7-be14b4ecc1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make trhe histogram of counts\n",
    "\n",
    "counts = np.bincount(counts_in_tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0ab585-4efa-448d-91ca-50ca3967a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(counts) == tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeca1484-c248-4b18-987d-0e442f3d725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.arange(len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e1ff47-8d27-4608-b450-7a0300107f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stairs(counts, np.append(values, values[-1]+1) - 0.5,\n",
    "           fill=True\n",
    "          )\n",
    "\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.savefig('DropDistribution.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278a45b2-180e-4b2c-bbf8-3b71b6b8933e",
   "metadata": {},
   "source": [
    "## study the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94df565e-b6a7-40ce-af48-80a42ee23f09",
   "metadata": {},
   "source": [
    "### mean\n",
    "The mean of the list of values is\n",
    "\\begin{equation}\n",
    "\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dfcd4a-55ff-4403-9622-0dd3ed1e24ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(counts_in_tiles)/len(counts_in_tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cfb011-046c-4704-a0ab-6be0c4265b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(counts_in_tiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef79e64-e009-4cbe-bec0-f0e2a6818a2c",
   "metadata": {},
   "source": [
    "The expected value of a distribution is\n",
    "\\begin{equation}\n",
    "\\mu = \\frac{1}{\\sum_{i = 1}^{n} w_i} \\sum_{i = 1}^{n} w_i x_i = \\sum_{i = 1}^{n} p_i x_i\n",
    "\\end{equation}\n",
    "with $w_i$ being the weigths of the individual values or $p_i$ being the $p_i$ being the probabilities such that $\\sum_{i = 1}^{n} p_i = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60cfe57-6567-4978-8eb5-800db41baf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d8e84-c32b-4a4e-87a9-bc13438d241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.sum(values*counts)/np.sum(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828c80c6-1839-4285-b030-a5a86a26145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d402cb0-4096-4bae-b654-87234ca67421",
   "metadata": {},
   "source": [
    "### variance\n",
    "The variance of the list of equally likely values is\n",
    "\\begin{equation}\n",
    "\\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n \\left( x_i - \\bar{x} \\right)^2\n",
    "\\end{equation}\n",
    "with $\\sigma$ being the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea5109a-4c3d-4a6b-9cb2-bb6d4724930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum((counts_in_tiles - np.mean(counts_in_tiles))**2)/len(counts_in_tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb29e918-c5eb-43ea-a3be-71a86ec8fc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(counts_in_tiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75047229-430a-435b-83d3-81652c71711b",
   "metadata": {},
   "source": [
    "Similarily, the variance of a distribution is\n",
    "\\begin{equation}\n",
    "\\sigma^2 = \\frac{1}{\\sum_{i = 1}^{n} w_i} \\sum_{i = 1}^{n} w_i (x_i - \\mu)^2 = \\sum_{i = 1}^{n} p_i (x_i - \\mu)^2\n",
    "\\end{equation}\n",
    "with $\\mu$ being the expected value (or mean) of the distribution,  $w_i$ being the weigths of the individual values or $p_i$ being the $p_i$ being the probabilities such that $\\sum_{i = 1}^{n} p_i = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e3aca4-c12a-4aa5-8d72-a9c6588b3547",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum((values-mu)**2*counts)/np.sum(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18e7e62-607c-4fb2-abe2-4ae9c013dbc9",
   "metadata": {},
   "source": [
    "### skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb7dabd-e434-4675-ac03-853f4b456d91",
   "metadata": {},
   "source": [
    "We can also calculate the third moment of the distribution\n",
    "\\begin{equation}\n",
    "\\mu_3 = \\sum_{i=1}^n p_i \\left( x_i - \\mu \\right)^3\n",
    "\\end{equation}\n",
    "with $p_i$ being the probabilities. For the list of numbers $p_i = 1/n$. This is related to the skewness and is a measure of the asymmetry of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff696e3-ccab-476f-a71c-039761163884",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_3 = np.sum((counts_in_tiles - np.mean(counts_in_tiles))**3)/len(counts_in_tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1491592b-5ee1-4940-a005-2e224644a3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a86a5b-5c48-4557-873d-c3d9a64e0392",
   "metadata": {},
   "source": [
    "There are different normalisations that can be used, '''scipy.stats''' implements the Fisher-Pearson coefficient of skewness,\n",
    "\\begin{equation}\n",
    "g_1 = \\frac{\\mu_3}{\\sigma^3}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b359d7-1ff4-44e6-bdb4-cfafcc0dc685",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_3/np.var(counts_in_tiles)**(3/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eae12a-234e-4bb4-81d4-4bfebbf8183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6554e832-9c96-4bc7-a541-f4988acd443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "skew(counts_in_tiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f3f8c6-44f8-44e8-8981-9b8c36f939e6",
   "metadata": {},
   "source": [
    "The details will not be discussed here. The main point is to show that the distribution is not symmetric, so it cannot be a normal distribution!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31524bfc-c46e-4cb9-8da5-41b241c0d848",
   "metadata": {},
   "source": [
    "### standard deviation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9747b0ad-73c6-42d5-92f1-61bf00bd038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.mean(counts_in_tiles)\n",
    "var = np.var(counts_in_tiles)\n",
    "sigma = np.std(counts_in_tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994a67b8-6a99-4bae-b2b1-85e8ec0e3908",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, var, sqrt(var), sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7545abb7-519e-4046-b67e-b5e181bba846",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = (counts_in_tiles - mu) / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccfcacd-8be8-4648-95e0-a8fd1c45f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dev, bins = np.arange(int(np.min(dev)), int(np.max(dev))+2))\n",
    "\n",
    "plt.xlabel('$(x - \\\\mu)/\\\\sigma$')\n",
    "\n",
    "#plt.yscale('log')\n",
    "\n",
    "plt.savefig('Deviation.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b240f1-8909-44a4-bd29-4bff20d91529",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.abs(dev) < 3)/len(counts_in_tiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db14d7a-df8b-48cd-a1c8-500cc85610d0",
   "metadata": {},
   "source": [
    "99.7% within $3\\sigma$ is what we expect for a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7a918f-67a1-4c75-96c3-7d9653695eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - 2*scipy.stats.norm.sf(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc23738d-13d4-4c0a-86ae-395f355ca059",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.abs(dev) < 1)/len(counts_in_tiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceff5f9-8843-49f2-a531-7b39dd744883",
   "metadata": {},
   "source": [
    "74% within $1\\sigma$ is higher than expected. But the distribution is not normal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084b1a2-086d-485b-b247-7807da890a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - 2*scipy.stats.norm.sf(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f087aa-6549-4733-a0a9-ec704eb30561",
   "metadata": {},
   "source": [
    "## Estimating the Rate\n",
    "\n",
    "Now we will try to estimate the true value $\\mu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f34b1d-1933-4aca-b39e-aa4dbb9cb932",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cb9a62-5608-4224-b750-9eb26749d9cc",
   "metadata": {},
   "source": [
    "from our measurements. We are looking for an uncertainty with a confidence level of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac26acc-33fd-4d85-88d7-147ecf2feeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_level = 1 - 2*scipy.stats.norm.sf(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f74cc6-28f8-4dd1-9dcf-e78a0363104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ad2c52-1eb7-4e74-990c-e93ad3c3298d",
   "metadata": {},
   "source": [
    "### single measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064ebc76-e292-4337-82e6-c115ffdb76bd",
   "metadata": {},
   "source": [
    "We can use the counts in each tile as a measurement for the overall rate. Let's take one standard deviation as the square root of the measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ac85e-f35a-4099-8bd3-efdcd2e42cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = (counts_in_tiles - mu)/sqrt(counts_in_tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a5fad-0f86-4e93-a314-56852c89a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b039c8-7142-4483-bcfd-149282dd5093",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.isinf(dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663b5b9b-9187-47af-98ba-fdc796abac0d",
   "metadata": {},
   "source": [
    "We see already that this fails in the cases where we have zero counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1448343-0bcb-4185-822c-23df8a75e97b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(dev, \n",
    "         bins = np.arange(int(np.min(dev[~np.isinf(dev)])), int(np.max(dev[~np.isinf(dev)]))+2)\n",
    "        )\n",
    "\n",
    "plt.xlabel('$(x - \\\\overline{x}) / \\\\sigma$')\n",
    "\n",
    "plt.savefig('Deviation_SingleEstimate.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad81ebb8-9052-45ea-b4a1-883d3c90a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.abs(dev) < 1)/len(counts_in_tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c865678b-895e-4875-a663-220e7d08d105",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.abs(dev) < 3)/len(counts_in_tiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b330e03c-5ca3-4ffc-bbe9-3227764aa210",
   "metadata": {},
   "source": [
    "This seems to be a bad estimate. In particular for the rare case of zero counts we do not even have an estimate of the uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdaabbf-040c-4b75-ae9a-a0c6d724d809",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sqrt(counts_in_tiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9252e725-641d-4bd3-abac-f893f41f4a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(sqrt(counts_in_tiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d821b05-cd7c-474a-af95-2d2e6ba69ad5",
   "metadata": {},
   "source": [
    "We will be able to do better than that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e35300-b1a3-4a7e-8215-d8413895311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.t.isf((1-confidence_level)/2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2262ec8-5562-4e74-985d-6fcf7d516888",
   "metadata": {},
   "source": [
    "### average over several measurements\n",
    "Let's make the measurement several times, i.e. we are counting several tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9733bf77-17cd-489f-a393-d39e74f06c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a07e2cc-7306-4c20-b5c6-5b5761b7af7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.reshape(counts_in_tiles, shape = (-1, sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757ffc8c-b4cc-4c1c-96ac-5154e3c2e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdc76f2-0052-480a-ac77-d99bad03dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f2d5e9-c0f9-4844-a6d4-ad638f0d8777",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_means = np.mean(samples, axis = (1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c97bd-0e58-45fa-ac16-48d098208943",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542cf0aa-8dab-4397-b7a7-b98d16ca5a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sample_means, bins = np.arange(int(np.min(sample_means)), int(np.max(sample_means))+2) -0.5)\n",
    "\n",
    "plt.xlabel(f'mean of {sample_size} measurements')\n",
    "\n",
    "plt.savefig('MeanOf10.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e950ab-ce16-41a4-b8b1-d0f2fba7be2f",
   "metadata": {},
   "source": [
    "The standard deviation of the estimate of the mean is\n",
    "\\begin{equation}\n",
    "\\sigma_\\text{mean} = t_{\\alpha, n - 1} \\frac{\\sigma_\\text{sample}}{\\sqrt{n}}\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "\\sigma_\\text{sample} = \\sqrt{\\frac{1}{n - 1} \\sum_{i=1}^n \\left( x_i - \\bar{x} \\right)^2}\n",
    "\\end{equation}\n",
    "is the sample's standard deviation and $n$ is the number of measurements used for the mean.\n",
    "The $n - 1$ compared to the distribution's standard deviation is the Bessel correction.\n",
    "\n",
    "$t_{\\alpha, n - 1}$ is the percentile of the Student's $t$ distribution. For a given confidence level $cf$ it is such that\n",
    "\\begin{equation}\n",
    "\\int_{-t_{\\alpha, n - 1}}^{t_{\\alpha, n - 1}} t_{n - 1} (x) dx = cf\n",
    "\\end{equation}\n",
    "We make use of the symmetry of this distribution and chose \n",
    "$$\n",
    "\\alpha = \\frac{1 - cf}{2}\n",
    "$$\n",
    "so that\n",
    "\\begin{equation}\n",
    "\\int_{-\\infty}^{t_{\\alpha, n - 1}} t_{n - 1} (x) dx = \\alpha\n",
    "\\end{equation}\n",
    "and we will use the inverse survival function of ```scipy.stat.t``` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1ee827-1267-4db5-b40f-36e14c376f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea8bd6f-f73e-4d2c-8bd1-c64ab3d4a60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_CL_N = scipy.stats.t.isf((1-confidence_level)/2, sample_size - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0814b32-b417-49b7-9315-f9d0f466a061",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_CL_N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a32d45-31a3-40e7-a075-c28c69c9e6bc",
   "metadata": {},
   "source": [
    "Note that for sufficiently large $n$ the $t$ distribution is close to a Gaussian and for a confidence level of 68\\% this number is close to 1. We will not discuss the $t$ distribution further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7c5f38-3c2a-4672-ae9a-0d87272cd50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sd = np.std(samples, axis = (1), ddof = 1) ## ddof = 1 for Bessel correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2b3fab-ab47-4c5f-998c-4371284902ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sd = t_CL_N * sample_sd/sqrt(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8eb44d-0459-49da-975c-9660ea505597",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801236e0-e8da-4ee3-b393-91413344d19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mean_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa947339-6038-43eb-a898-7714e698c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_devs = (sample_means - mu)/mean_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a7d98e-eaac-405e-a241-00f48fe746fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_devs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82df46b7-4ba4-46ff-87fe-7ee7cb84415b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(sample_devs)\n",
    "\n",
    "#plt.yscale('log')\n",
    "plt.xlabel('$(x - \\\\overline{x}) / \\\\sigma$')\n",
    "\n",
    "\n",
    "plt.savefig('Deviation_MultipleObservation.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29282c1d-b209-4dc6-9d3e-8fc9baf73019",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.abs(sample_devs) < 1)/len(sample_devs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b065a4-d9e9-482a-a2c9-c06540e9a230",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.abs(sample_devs) < 3)/len(sample_devs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e918e2e-2b7d-4bfc-9c90-fa94a76e13ba",
   "metadata": {},
   "source": [
    "This is very close to what we have expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2240cf-4ef8-49cc-a3f8-4abf13801168",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(mean_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ba745-caf8-413f-8ef5-e709452f2ead",
   "metadata": {},
   "source": [
    "Our typical uncertainty from this method is the following. We will keep this and see later if we can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c42bd6-c483-47f6-bc81-0d7b87f4e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_from_mean = np.mean(mean_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b36e6dd-4330-42a7-bada-105e45fcdf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_from_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f4a674-21af-4a32-9b8c-2537f7ef221d",
   "metadata": {},
   "source": [
    "## Poisson statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4383446c-fd9e-448b-a05f-4d02811d43a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3b5929-e48e-464d-860c-416dd436095f",
   "metadata": {},
   "source": [
    "We have seen earlier that the distribution of counts in the tiles is not symmetric, has a mean of $\\mu$ and a standard deviation of $\\sqrt{\\mu}$. This is the Poisson distribution.\n",
    "\n",
    "The probability to count $n$ events when the mean (the expectation) is $\\mu$ is given by :\n",
    "\\begin{equation}\n",
    "\\text{prob}(n|\\mu) = \n",
    "\\frac{\\mu^n}\n",
    "{n!}\n",
    "e^{-\\mu}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff60370-c8a1-468d-bcbe-669a605e5ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stairs(counts, np.append(values, values[-1]+1) - 0.5,\n",
    "           fill=True,\n",
    "           label = 'observations'\n",
    "          )\n",
    "\n",
    "plt.plot(values,\n",
    "         scipy.stats.poisson.pmf(values, mu) *np.sum(counts),\n",
    "         marker = 'o',\n",
    "         label = 'Poisson ($\\\\mu = 10$)'\n",
    "        )\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('Poisson.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd76774d-fa25-4512-9b5f-5b74d79e0bf3",
   "metadata": {},
   "source": [
    "What is the probability of measuring exactly mu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d24528-d70d-4dbd-9fdc-706618f94f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.poisson.pmf(mu, mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c2fa18-a5af-418f-a8e7-6f7db3358afc",
   "metadata": {},
   "source": [
    "while we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f48a476-a099-41db-b963-9edd18211d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "f'{counts[values == mu]} / {np.sum(counts)} = {counts[values == mu]/np.sum(counts)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a55ea1-3d4f-47a4-bc93-5dd01508e819",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.poisson.pmf(100,100)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81817256-ea26-4fd0-8ded-3180e1def19c",
   "metadata": {},
   "source": [
    "What is the probability of having a completely empty tile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d3cc3-cb0a-4562-a59e-9c451e352548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scipy.stats.poisson.pmf(0, mu)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caabc73b-cb31-4f43-b4aa-cc83adc6dd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "f'{counts[values == 0]} / {np.sum(counts):1.0e} = {counts[values == 0]/np.sum(counts)*100}%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eba6a11-5bbe-4086-b721-017fd3e1a54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.poisson.pmf(0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3e7b29-4c72-4114-a76e-a1047d2c68ee",
   "metadata": {},
   "source": [
    "What is the probability of measuring less than or equal the mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9296bdea-13c2-4bcb-a3f3-bb851b8d511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.poisson.cdf(mu, mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9064d7ed-1921-47ff-bb53-c5c0905528a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f'{np.sum(counts[values <= 10])} / {np.sum(counts):1.0e} = {np.sum(counts[values <= 10])/np.sum(counts)*100}%'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5be28ef-041f-4d70-b26f-9afa2090d74e",
   "metadata": {},
   "source": [
    "What is the probability of measuring more than $\\mu$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5c8a0c-a570-4b85-b02f-08a60df36707",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.poisson.sf(mu, mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da11222b-602a-4396-b001-32b17bf5742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f'{np.sum(counts[values > 10])} / {np.sum(counts):1.0e} = {np.sum(counts[values > 10])/np.sum(counts)*100}%'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2db8c3e-b870-4de9-812e-fe4612268de6",
   "metadata": {},
   "source": [
    "What is the probability of measuring higher than our maximum value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6357ae1-bdbe-4431-8988-9961dd1a4118",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478c387a-3989-4ba4-a9cc-4ef84ec8b2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.poisson.sf(np.max(values), mu) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e8e517-4c79-4070-af7a-199e9c6213b8",
   "metadata": {},
   "source": [
    "## Likelihood Profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c7eeec-fb92-44a0-b625-0d462f2b6506",
   "metadata": {},
   "source": [
    "We have ten consecutive measurements, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30bdc0c-f727-41a4-94c1-59e90c0525f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833d8d84-d94f-4801-a2ff-4d07b6435904",
   "metadata": {},
   "source": [
    "We know the probability of the counts in each individual tile for a given rate. The overall probability is the product of that:\n",
    "\\begin{equation}\n",
    "\\mathcal{L} = \\text{prob} = \\prod_{i = 1}^{i=n} \\text{prob}(x_i|\\mu)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f191e3cb-2882-4669-83f9-ad356a354f26",
   "metadata": {},
   "source": [
    "Let's calculate the probability for all of our samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd8f89d-6f36-4012-a0b5-6c383ccb817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.prod(scipy.stats.poisson.pmf(samples, mu), axis = (1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f4ef7e-5f0f-4ed4-8719-40b72f03e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log10(np.prod(scipy.stats.poisson.pmf(samples, mu), axis = (1))), log = True)\n",
    "\n",
    "plt.xlabel('$\\\\log_{10} \\\\mathcal{L}$')\n",
    "\n",
    "plt.savefig('Ldistribution.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9571e691-0f4b-4eb4-a5ab-acbdec8ca8b8",
   "metadata": {},
   "source": [
    "Now let's scan the probability for different values of $\\mu$ around the true value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a56f646-906b-4315-9803-48dc57eadfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = np.arange(mu - 3, mu + 3, step = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6912be6b-535e-41ca-8e16-d041404e496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.prod(np.array(list(map(lambda mu : scipy.stats.poisson.pmf(samples[0], mu), mus))), axis = (1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca5bad4-fb1b-41f5-8f1e-d4b5685edd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023331ab-095b-4a1e-8ff4-f0a503299690",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(list(map(\n",
    "    lambda sample : np.prod(np.array(list(map(lambda mu : scipy.stats.poisson.pmf(sample, mu), mus))), axis = (1)), \n",
    "    samples[:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac638b7-6b66-49df-aab4-11d776059f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38471b5-2fa9-4507-af5b-ec2ad2ec8c33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(mus, x[0])\n",
    "\n",
    "plt.xlabel('$\\\\mu$')\n",
    "plt.ylabel('$\\\\mathcal{L}$')\n",
    "\n",
    "plt.savefig('Lprofile.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb9a1c6-d824-4b1f-9bb9-1ef63e1a97fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mus, x[18])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec9e5a2-26e1-46ba-924f-09be55237c7d",
   "metadata": {},
   "source": [
    "This function has a very clear maximum, which we can use to estimate the true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f795662a-806b-43a1-be86-f4a52175849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = plt.plot(mus, np.transpose(x[:10]))\n",
    "\n",
    "#plt.yscale('log')\n",
    "\n",
    "plt.xlabel('$\\\\mu$')\n",
    "plt.ylabel('$\\\\mathcal{L}$')\n",
    "\n",
    "plt.savefig('Lprofile2.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad19977-803f-45ce-863c-d0734f697fde",
   "metadata": {},
   "source": [
    "Finding the maximum of this profile will give us an estimate of the rate from the sample. This would need the first derivative. Using the second derivative we could even obtain information on the uncertainty. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88324425-f8b2-433b-8a57-4b57e6c4bd4c",
   "metadata": {},
   "source": [
    "### Log Likelihood\n",
    "As we are looking for the maximum of the likelihood distribution and the logarithm is strictly increasing, we can also work with the logarithm of the likelihood. This will also simplify certain calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967bbd1a-f995-4d35-b64f-015f26c36fcd",
   "metadata": {},
   "source": [
    "In particular the product of the probabilities transforms into a sum:\n",
    "$$\n",
    "\\mathcal{l}  = \\ln\\mathcal{L} = \\ln \\left( \\prod_{i = 1}^{i=n} \\text{prob}(x_i|\\mu) \\right) =   \\sum_{i = 1}^{i=n} \\ln \\left( \\text{prob}(x_i|\\mu)  \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5316e105-eea0-4ade-b24d-73027d7ce471",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = np.array(list(map(\n",
    "    lambda sample : np.sum(np.array(list(map(lambda mu : np.log(scipy.stats.poisson.pmf(sample, mu)), mus))), axis = (1)), \n",
    "    samples[:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2571eb-8a9f-4838-bf98-1a5825a05c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1650e176-f479-4d0f-b2fc-131739c2716d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(mus, ls[0])\n",
    "\n",
    "plt.xlabel('$\\\\mu$')\n",
    "plt.ylabel('$\\\\ln \\\\mathcal{L}$')\n",
    "\n",
    "plt.savefig('lnLprofile.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f28281-e03a-45ad-b475-55a3dc8867a5",
   "metadata": {},
   "source": [
    "For the Poisson distribution we can simplify even further:\n",
    "\\begin{align*}\n",
    "\\mathcal{l} & = \\ln\\mathcal{L}\n",
    "= \\sum_{i = 1}^{i=N} \\ln \\left( \\frac{\\mu^n_i}\n",
    "{n_i!}\n",
    "e^{-\\mu}  \\right)\n",
    "\\\\\n",
    "{} &= \\sum_{i = 1}^{i=N}\n",
    "\\left(\n",
    "- \\mu + n_i \\ln{\\left(\\mu \\right)} - \\ln{\\left(n_i! \\right)}\n",
    "  \\right)\n",
    "\\\\\n",
    "{} &= - N \\mu  + \\ln{\\left(\\mu \\right)} \\sum_{i = 1}^{i=N}\n",
    "   n_i  -\n",
    "  \\sum_{i = 1}^{i=N} \\ln{\\left(n_i! \\right)}\n",
    "\\end{align*}\n",
    "\n",
    "The last term is a constant and does depend only on our data set, and not on $\\mu$. If we want to study the likelihood profile in dependance of $\\mu$ we can ignore this term and define a test statistic as\n",
    "$$\n",
    "TS = - N \\mu  + \\ln{\\left(\\mu \\right)} \\sum_{i = 1}^{i=N}\n",
    "   n_i\n",
    "$$\n",
    "Which is close to the Cash statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e703e51a-c6de-477a-9d96-9ad19254627c",
   "metadata": {},
   "source": [
    "If we want to find the maximum of this distribution we need to calculate the value $\\mu$ for which the deriviative vanishes. The derivative of $\\ln{\\mathcal{l}}$ is\n",
    "$$\n",
    "\\frac{d~\\ln{\\mathcal{L}}}{d\\mu} = - N + \\frac{\\sum_{i=1}^{N} {n}_{i}}{\\mu}\n",
    "$$\n",
    "\n",
    "so that \n",
    "$$\n",
    "\\mu = \\frac{\\sum_{i=1}^{N} {n}_{i}}{N}\n",
    "$$\n",
    "which is simply the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d380552-8f49-40f5-a19e-d7605e00fa36",
   "metadata": {},
   "source": [
    "While this looks like a fancy way to calculate the mean the likelihood profile is much more powerful. In our example the the expectation value in each tile is exactly the same. This is not always the case. Imagine tiles of different sizes, or tiles being measurements with different durations. You can still calculate the probability for each tile and get the likelihood profile.\n",
    "\n",
    "The likelihood profile may even depend on several parameters, and an analytical solution or derivitative might no exist. In this case the negative of our test statistics, $-ts$, can be numerically minimised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3553e586-2569-42ae-9928-b9b6b8fcd0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts (mu) :\n",
    "    return - (np.log(mu)*np.sum(counts_in_tiles) -tiles*mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c2f00a-45ee-4ac5-939c-ab2aedae7c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optres = scipy.optimize.minimize(ts, [1],\n",
    "                                 bounds=[(0,np.inf)], \n",
    "                                 hess='2-point'\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c01568-1f5a-46fb-aef4-a73d716696dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56d3c50-1f32-4d14-9a92-9ab9787f95f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optres.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c62e91-75f8-479a-9975-cacd80533c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mus = (np.sum(samples, axis = 1))/samples.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcad999-bd96-4560-995d-3dbd29c76036",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(sample_mus, bins = np.arange(int(np.min(sample_mus)), int(np.max(sample_mus))+2) -0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e5754c-f9ec-422c-ad61-83df27522344",
   "metadata": {},
   "source": [
    "### Estimating Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc729d-3736-4a20-b452-494ddd89c8d5",
   "metadata": {},
   "source": [
    "#### Hessian matrix\n",
    "\n",
    "The negative of the inverse of the Hessian matrix of the logarithm of the likelihood is an estimate for the covariance of the estimated parameters. (This is another reason to use the log of the likelihood.) Certain conditions have to be met, in particular the likelihood has to be normally distributed around the maximum.\n",
    "\n",
    "In our example we have only one single parameter and the Hessian is simply the second derivative of $\\ln{\\mathcal{L}}$ which is\n",
    "$$\n",
    "h = \\frac{d^2~\\ln{\\mathcal{L}}}{d\\mu^2} = - \\frac{\\sum_{i=1}^{N} {n}_{i}}{\\mu^{2}}\n",
    "$$\n",
    "\n",
    "So the standard deviation of $\\mu$ is\n",
    "$$\n",
    "\\sigma_\\mu = \\frac{1}{\\sqrt{-h}} = \\frac{\\mu}{\\sqrt{\\sum_{i=1}^{N} {n}_{i}}}\n",
    "$$\n",
    "It is the mean, as expected from a Poisson distribution, divided by the total number of counts. There more events, the better the estimate!\n",
    "\n",
    "It can also be written as\n",
    "$$\n",
    "\\sigma_\\mu = \\frac{\\sqrt{\\sum_{i=1}^{N} {n}_{i}}}{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06468f78-b0d3-4514-8eb5-b3c5c6fe12cf",
   "metadata": {},
   "source": [
    "In the case that the derivative cannot be obtained analytically the inverse Hessian can be obtained from a numerical minimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aac59a-e5ec-4b31-816d-03a6997c52ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.sum(counts_in_tiles))/tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03940641-375d-42c3-9b98-07e93741db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optres.hess_inv.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cae1a1-3b44-45b6-af81-3ee32faae61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt(optres.hess_inv.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc1bfe1-2af9-4dd7-b206-d52bef1e3795",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726d69e6-c5de-42be-96cc-aab239563cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_sd_hessian = np.sqrt(np.sum(samples, axis = 1))/samples.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef2dce5-d2d9-4b2d-899d-5d421390e3e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(mu_sd_hessian)\n",
    "\n",
    "plt.yscale('log')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1f932d-7144-4d8a-ae6c-66684242ef05",
   "metadata": {},
   "source": [
    "This looks very good. The spread is much lower then what we have found before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b13f50-bc24-434b-a302-d6d0abc530e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_devs_hessian = (sample_mus - mu)/mu_sd_hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3807894b-a4e3-4b51-a212-cb1fd0a66daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_devs_hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4368cea4-78bf-4bed-9ef9-d82314ee8ac0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(sample_devs_hessian)\n",
    "\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('$(x - \\\\overline{x}) / \\\\sigma$')\n",
    "\n",
    "plt.savefig('Deviation_Hessian.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b886e2-af58-43c4-aa9e-056c8e159325",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.abs(sample_devs_hessian) < 1)/len(sample_devs_hessian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c493e102-84c1-4fa2-85b4-b06ec69841d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.abs(sample_devs_hessian) < 3)/len(sample_devs_hessian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197d577e-c6af-478c-8464-736da4007dac",
   "metadata": {},
   "source": [
    "This is very much what we have expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d215fb75-ec07-4bac-b3cc-a06b598a7629",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(mu_sd_hessian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6ce6d8-f3ac-4e29-9ef8-42452cbea288",
   "metadata": {},
   "source": [
    "Our typical uncertainty from this method is the following. We will keep this and see later if we can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf68e14-42c5-4bb5-9b97-cb509357a49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_from_hessian = np.mean(mu_sd_hessian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd173075-a1c6-4d40-81c7-9452637c7f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_from_hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e027215-4f56-4103-844b-82e7f987dc23",
   "metadata": {},
   "source": [
    "We have some improvement from our mean method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53c496a-a0f2-4177-8157-49da3e95b012",
   "metadata": {},
   "source": [
    "#### Using Wilks' theorem\n",
    "\n",
    "So far we have used the first and second derivatives of $\\ln{\\mathcal{L}}$ or at least assumed that the profile is normal distributed around the maximum. We now use a different approach which would also work for other profiles.\n",
    "\n",
    "Wilks' theorem states that the ratio of the likelihoods of two models $0$ and $1$\n",
    "$$\n",
    "D = -2 \\ln{\\frac{\\mathcal{L_0}}{\\mathcal{L_1}}}\n",
    "$$\n",
    "approaches asymptotically when the sample sizes goes to infinity a $\\chi^2$ distribution and will give the probabilty of model 1 being a statistical fluctuation of model 0, the null hypothesis. \n",
    "This can be written as\n",
    "$$\n",
    "D = -2 ( \\ln {\\mathcal{L_0}} - \\ln {\\mathcal{L_1}}) = 2 ( \\ln {\\mathcal{L_1}} - \\ln {\\mathcal{L_0}})\n",
    "$$\n",
    "which is another reason why we use the logarithm of the likelihood and explains the factor 2 in the Cash statistics.\n",
    "The models have to be nested models: Model 1 is the alternative model, and model 0 is a special case of the alternative model, i.e. it is Model 1 with a subset of the parameters frozen. The degrees of freedom is then the difference of the number of free parameters in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988acc4a-411f-4629-b2f3-3cf561f95676",
   "metadata": {},
   "source": [
    "In order to construct a confidence level we will scan the parameter space (the allowed values for $\\mu$) and calculate how close they are to the best fit value $\\mu_0$, obtained from the maximum of the likelihood distribution. The null hypothesis is that $\\mu$ is fixed to the value under investigation and all other parameters (in the case of a multiparameter model) are optimised (likelihood maximised). The alternative hypothesis is with all parameters free and optimised, i.e. we compare with the best-fit parameter $\\mu_0$.\n",
    "\n",
    "This way we can construct a test statistic profile, $D(\\mu)$. Obviousily, $D(\\mu_0) = 0$. Further on, as the likelihood is maximal at $\\mu_0$, $D$ positive for all other values of $\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739fc714-cfbf-4940-aab3-c68543ea7e4b",
   "metadata": {},
   "source": [
    "Let's take a look at it for a Poisson distribution:\n",
    "\n",
    "$$\n",
    "D = 2 \\left(N \\mu - \\ln{\\left(\\mu \\right)} \\sum_{i=1}^{N} {n}_{i} + \\sum_{i=1}^{N} \\ln{\\left({n}_{i}! \\right)}\\right) + 2 \\left(- N \\mu_{0} + \\ln{\\left(\\mu_{0} \\right)} \\sum_{i=1}^{N} {n}_{i} - \\sum_{i=1}^{N} \\ln{\\left({n}_{i}! \\right)}\\right)\n",
    "$$\n",
    "\n",
    "We see that the term $\\sum_{i=1}^{N} \\ln{\\left({n}_{i}! \\right)}$ cancels out. This is the reason for ignoring it in the Cash statistics.\n",
    "\n",
    "\\begin{align*}\n",
    "D  &= 2 N \\mu - 2 N \\mu_{0} + 2 \\left(- \\ln{\\left(\\mu \\right)} + \\ln{\\left(\\mu_{0} \\right)}\\right) \\sum_{i=1}^{N} {n}_{i} \\\\\n",
    "{} &= 2 N \\mu - 2 N \\mu_{0} + 2 n \\left(- \\ln{\\left(\\mu \\right)} + \\ln{\\left(\\mu_{0} \\right)}\\right)\n",
    "\\\\\n",
    "{} & = 2 N \\mu + 2 n \\left(- \\ln{\\left(\\mu \\right)} + \\ln{\\left(\\frac{n}{N} \\right)}\\right) - 2 n\n",
    "\\end{align*}\n",
    "\n",
    "The number of degrees of freedom in this case is one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccc1e40-9f12-44fd-a919-aee9746298d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def D(mu, N, n) :\n",
    "    \"\"\"N measurements, sum of counts = n\"\"\"\n",
    "    \n",
    "    return (2*N*mu + 2*n*(-np.log(mu) + np.log(n/N)) - 2*n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282c35f0-6e84-4a5a-be4a-fa19af37ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691ada1a-8e81-428b-9542-36ef4ebe2df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "D(mus, samples.shape[1], np.sum(samples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705d2361-2273-433d-959b-19af84ba525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ds = list(map(lambda sum : D(mus, samples.shape[1], sum), np.sum(samples, axis = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd9438e-cd53-46c6-aac4-8d002a612165",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mus, Ds[0])\n",
    "#plt.plot(mus, Ds[1])\n",
    "\n",
    "plt.ylim(0)\n",
    "\n",
    "plt.ylabel('$D$')\n",
    "\n",
    "plt.xlabel('$\\\\mu$')\n",
    "\n",
    "plt.savefig('Dprofile.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac78ac03-e207-4756-bbec-8e059f0681f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.chi2.ppf(0.68, df = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b256a96c-0429-4307-90e5-d9dcc67df5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mus, Ds[0])\n",
    "#plt.plot(mus, Ds[1])\n",
    "\n",
    "plt.ylim(0)\n",
    "\n",
    "plt.ylabel('$D$')\n",
    "\n",
    "plt.xlabel('$\\\\mu$')\n",
    "\n",
    "secax_y = plt.gca().secondary_yaxis('right', \n",
    "                             functions=(lambda D : scipy.stats.chi2.cdf(D, df = 1), \n",
    "                                        lambda p : scipy.stats.chi2.ppf(p, df = 1)\n",
    "                                        )\n",
    "                            )\n",
    "secax_y.set_ylabel('$\\\\chi^2$ probability')\n",
    "\n",
    "\n",
    "#plt.savefig('Dprofile.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a462c11b-e3b7-442e-91ec-12a77b45110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mus,\n",
    "         scipy.stats.chi2.cdf(Ds[0], df = 1)\n",
    "        )\n",
    "\n",
    "plt.ylabel('$\\\\chi^2$ probability')\n",
    "\n",
    "plt.xlabel('$\\\\mu$')\n",
    "\n",
    "plt.ylim(0,0.9999)\n",
    "\n",
    "\n",
    "secax_y = plt.gca().secondary_yaxis('right', \n",
    "                             functions=(lambda p : scipy.stats.chi2.ppf(p, df = 1),\n",
    "                                        lambda D : scipy.stats.chi2.cdf(D, df = 1)\n",
    "                                        )\n",
    "                            )\n",
    "secax_y.set_ylabel('$D$')\n",
    "\n",
    "secax_y.set_yticks([0,1,2,3,4,5])\n",
    "\n",
    "plt.savefig('Chi2Probability.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2e7341-bc01-4290-8327-e6c9674e58ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.chi2.cdf([0, 1, 2, 3, 4], df = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff64bd1-e742-4b19-9957-cac0cdc511a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.chi2.ppf(1 - 2*scipy.stats.norm.sf([1, 2, 3]), df = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd3a690-7b32-4d3c-9f86-c29e4b1efdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - 2*scipy.stats.norm.sf([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31263a1-0b50-44ff-a7da-c09717556146",
   "metadata": {},
   "source": [
    "The individual values follow a $\\chi^2$ distribution with 1 degree of freedom. 68.27% of the values in a $\\chi^2$ distribution are lower than 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5320606-4e49-472a-85e2-8ca0c7b5f523",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.chi2.ppf(confidence_level,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6741bad-be9c-47ed-8e99-b1491a7d9326",
   "metadata": {},
   "source": [
    "So the limits of our confidence interval are where $D$ reaches 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d5b1a2-6efa-4df2-b3ec-3e540b7090fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.optimize.fsolve(lambda mu : D(mu, sample_size, np.sum(samples[0])) - 1, np.mean(samples[0])*np.array([0.9, 1.1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b600a48b-d969-4da3-b18d-5e5857fc67b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conf_interval = np.array(list(\n",
    "    map(lambda x : scipy.optimize.fsolve(lambda mu : D(mu, sample_size, np.sum(x)) - 1, np.mean(x)*np.array([0.9, 1.1])),\n",
    "                                         samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a6ad08-6c6c-4cd6-82f4-0a7e0b8706b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conf_interval_3s = np.array(list(\n",
    "    map(lambda x : scipy.optimize.fsolve(lambda mu : D(mu, sample_size, np.sum(x)) - 9, np.mean(x)*np.array([0.9, 1.1])),\n",
    "                                         samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f16787-408a-4ec0-bbb9-c980e91de6b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conf_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e2f765-7144-4d11-9ad1-460abd0f3103",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e572d2e1-2faa-4866-8155-2a40117d3369",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec424c68-8c9f-470e-a1e2-3b41befd7f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "mus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5048ad11-c43e-4365-b080-21b50b77722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "\n",
    "plt.plot(mus, Ds[index])\n",
    "\n",
    "maxD = np.nanmax(Ds[index])\n",
    "\n",
    "plt.ylim(0)\n",
    "\n",
    "plt.ylabel('$D$')\n",
    "\n",
    "plt.xlabel('$\\\\mu$')\n",
    "\n",
    "plt.vlines(sample_mus[index], 0, maxD,\n",
    "           ls = 'dotted', color = 'green',\n",
    "           label = 'mean'\n",
    "          )\n",
    "\n",
    "plt.vlines(sample_mus[index] + np.array([mu_sd_hessian[index], -mu_sd_hessian[index]]), 0, maxD,\n",
    "           ls = 'dotted', color = 'red',\n",
    "           label = 'Hessian uncertainty'\n",
    "          )\n",
    "\n",
    "plt.vlines(conf_interval[index], 0, maxD,\n",
    "           ls = 'dotted', color = 'darkred',\n",
    "           label = \"Wilk's uncertainty\"\n",
    "          )\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlim(sample_mus[index] + 2*np.array([-mu_sd_hessian[index], +mu_sd_hessian[index]]))\n",
    "\n",
    "plt.savefig('Dprofile_withUnc.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d412b-957a-4e55-b276-df5ca97d1a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_interval < mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d49a7b8-719b-4c75-baad-71bed9cda93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.array(list(map(lambda x : np.logical_and(x[0]<= mu, x[1] >= mu) , conf_interval))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae17d54-c1b0-42e0-b043-748c3b22805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.array(list(map(lambda x : np.logical_and(x[0]<= mu, x[1] >= mu) , conf_interval_3s))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96af70e-d3b5-48c4-a6d7-1f7889acb47e",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d890f78-1ffd-4841-a84a-86e601f0ab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(counts_in_tiles) % 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c042f12-481e-4865-9276-7e22d613750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples2 = np.reshape(counts_in_tiles[:-(len(counts_in_tiles) % 6)], shape = (-1, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca1c89a-6b8a-47bb-b251-87ddcb5b8ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3b2943-97ef-4b77-8058-1a01d96ecdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778d490e-7d62-429d-a838-16cb76ca928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples2[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d745eafa-6b9f-460b-badf-db914fcfef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(samples2[0][1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942f79c-293a-49db-b221-e5668e98f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(\n",
    "samples2[0][3:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c367cb-df28-46bf-a157-58a0a6bd08c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "exsamples = np.array([samples2[:,0], np.sum(samples2[:,1:3], axis = 1), np.sum(samples2[:,3:], axis = 1)]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cdcee3-24e8-49c1-bf26-d81259716e22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d901c7ee-15ab-4ef3-ad9e-9cdd32e91664",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(exsamples[1:4,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc498ac-b103-4c56-bf18-426c5103a0a1",
   "metadata": {},
   "source": [
    "### $A_x$ par marginalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f5020e-28e9-476a-a2cd-57aed26d7dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check_Ax(counts, A_x, sigma = 1) :\n",
    "\n",
    "    n_1 = counts[0]\n",
    "    n_2 = counts[1]\n",
    "    n_x = counts[2]\n",
    "\n",
    "    Ax_best = (3*n_x/(n_1 + n_2))\n",
    "\n",
    "    Ax_std = (3*sqrt(n_x)*sqrt(-1/(-n_1 - n_2)**3)*sqrt(n_1 + n_2 + n_x))\n",
    "\n",
    "    return abs(Ax_best-A_x) < sigma*Ax_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5daa00c-99c4-4c95-923d-46ad1a88dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(list(map( lambda counts : Check_Ax (counts, A_x = 3), exsamples))) / exsamples.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cb414a-5612-45c6-b54e-5bdee52ae0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(list(map( lambda counts : Check_Ax (counts, A_x = 3, sigma = 3), exsamples))) / exsamples.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7ef779-9d47-4313-8a3b-75b20e6363e1",
   "metadata": {},
   "source": [
    "### $\\mu$ par marginalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe54228-1e59-42eb-97f7-80bb3a4ce75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check_mu(counts, mu, sigma = 1) :\n",
    "\n",
    "    n_1 = counts[0]\n",
    "    n_2 = counts[1]\n",
    "    n_x = counts[2]\n",
    "\n",
    "    mu_best = ((1/3)*n_1 + (1/3)*n_2)\n",
    "\n",
    "    mu_std = ((1/3)*sqrt(n_1 + n_2))\n",
    "\n",
    "    return abs(mu_best-mu) < sigma*mu_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3095eae5-5a08-4ec1-9858-d7f57b7d9d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(list(map( lambda counts : Check_mu (counts, mu = 10), exsamples))) / exsamples.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759c5c65-c277-473d-b84e-31f08a395b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(list(map( lambda counts : Check_mu (counts, mu = 10, sigma = 3), exsamples))) / exsamples.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95896bee-04d4-47ed-b2db-16a874b284c1",
   "metadata": {},
   "source": [
    "### 2d Likelihood Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9cfce8-c111-42d9-8141-9296cb2502ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeltaTS (counts, mu, A_x) :\n",
    "\n",
    "    n_1 = counts[0]\n",
    "    n_2 = counts[1]\n",
    "    n_x = counts[2]\n",
    "    \n",
    "    return (2*A_x*mu + 6*mu - 2*n_1*math.log(mu) + 2*n_1*math.log(n_1 + n_2) - 2*n_1*math.log(3) - 2*n_1 - 2*n_2*math.log(mu) + 2*n_2*math.log(n_1 + n_2) - 2*n_2*math.log(3) - 2*n_2 - 2*n_x*math.log(A_x) - 2*n_x*math.log(mu) + 2*n_x*math.log(n_x) - 2*n_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9507ae31-22cb-4e51-bdd4-3dec25e43a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeltaTS (exsamples[0], mu = 10, A_x = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacf9d03-bd9d-4c56-890a-cf0f1d373639",
   "metadata": {},
   "outputs": [],
   "source": [
    "TS = np.array(list(map( lambda counts : DeltaTS (counts, mu = 10, A_x = 3), exsamples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a318853a-5083-4004-afb2-69160ad3c293",
   "metadata": {},
   "outputs": [],
   "source": [
    "TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfd66f9-2efa-4422-9466-fcd9c2b6f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(TS < scipy.stats.chi2.ppf(1 - 2*scipy.stats.norm.sf(1), df = 2)) / exsamples.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efa63c5-306a-4256-8d13-f81c5669ab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(TS < scipy.stats.chi2.ppf(1 - 2*scipy.stats.norm.sf(3), df = 2)) / exsamples.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aae046-29bc-4d96-bbe3-b8c164b4c664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
